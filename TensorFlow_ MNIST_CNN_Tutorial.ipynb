{"cells":[{"metadata":{},"cell_type":"markdown","source":"Adapted from [Keras GitHub Example](http://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) and [Yassine Ghouzam's NB](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n\nTensorFlow 1.x --> TensorFlow 2.x\n\nSciKit Learn --> TensorFlow"},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"This tutorial is an introduction to Convolutional Neural Networks using TensorFlow 2.x Keras API. The dataset that we will work it is the MNIST dataset, a dataset of handwritten digits 0-9, and we will use a Sequential CNN to predict which digit was drawn.\n\nThis model reaches 99.3% accuracy.\n\nTo prepare our notebook, run the next cell to import the necessary packages and change the accelerator from ```None``` to ```GPU```."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint(tf.__version__)","execution_count":1,"outputs":[{"output_type":"stream","text":"2.1.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preprocessing\n\nBefore building any ML model, it is important to preprocess the data. In fact, data preprocessing will generally take up the most time in any ML pipeline. The following module goes over the steps to preprocess the MNIST dataset for our purposes."},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Load Data\n\nOur first step is to load the data and divide it into a training and testing dataset. The MNIST dataset can be downloaded directly from TensorFlow and has already been divided. Run the next cell to import the data.\n\n``` x_train ``` is the dataset of 28x28 images of handwritten digits that the model will be trained on.\n\n```y_train``` is the dataset of labels that correspond to ```x_train```. \n\n``` x_test ``` is the dataset of 28x28 images of handwritten digits that the model will be tested on.\n\n```y_test``` is the dataset of labels that correspond to ```x_test```. "},{"metadata":{"trusted":true},"cell_type":"code","source":"mnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Run the following code to see the counts of each digit present in our training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y_train)","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7fb9d531e4d0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"There are similar counts for each digit. This is good as the model will have enough images for each class to train the features for each class. There is no need to downsample or upweigh."},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Check for NaN Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.isnan(x_train).any()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.isnan(x_test).any()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"There are no NaN values in our dataset. There is no need to preprocess the data to deal with Nan's."},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Normalization and Reshaping\n\nSince the values in our ```x_train``` dataset are 28x28 images, our input shape must be specified so that our model will know what is being inputed.\n\nThe first convolution layer expects a single 60000x28x28x1 tensor instead of 60000 28x28x1 tensors.\n\nModels generally run better on normalized values. The best way to normalize the data depends on each individual dataset. For the MNIST dataset, we want each value to be between 0.0 and 1.0. As all values originally fall under the 0.0-255.0 range, divide by 255.0.\n\nRun the following cell to define the ```input_shape``` and to normalize and reshape the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape = (28, 28, 1)\n\nx_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\nx_train=x_train / 255.0\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\nx_test=x_test/255.0","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Label Encoding\n\nThe labels for the training and the testing dataset are currently categorical and is not continuous. To include categorical dataset in our model, our labels should be converted to one-hot encodings.\n\nFor example, ```2``` becomes ```[0,0,1,0,0,0,0,0,0,0]``` and ```7``` becomes ```[0,0,0,0,0,0,0,1,0,0]```.\n\nRun the following cell to transform the labels into one-hot encodings"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\ny_test = tf.one_hot(y_test.astype(np.int32), depth=10)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Visualize Data\n\nRun the following cell to visualize an image in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x_train[100][:,:,0])\nprint(y_train[100])","execution_count":8,"outputs":[{"output_type":"stream","text":"tf.Tensor([0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"The image is an image of a handwritten ```5```. The one-hot encoding holds the value of ```5```."},{"metadata":{},"cell_type":"markdown","source":"# 3. CNN\n\nIn this module, we will build our CNN model."},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Define the Model\n\nRun the following cell to define ```batch_size```, ```num_classes```, and ```epochs```. Try changing the values and test how different values affect the accuracy of the CNN model."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nnum_classes = 10\nepochs = 5","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the following cell to build the model. The model contains various layers stacked on top of each other. The output of one layer feeds into the input of the next layer.\n\nConv2D layers are convolutions. Each filter (32 in the first two convolution layers and 64 in the next two convolution layers) transforms a part of the image (5x5 for the first two Conv2D layers and 3x3 for the next two Conv2D layers). The transformation is applied on the whole image.\n\nMaxPool2D is a downsampling filter. It reduces a 2x2 matrix of the image to a single pixel with the maximum value of the 2x2 matrix. The filter aims to conserve the main features of the image while reducing the size.\n\nDropout is a regularization layer. In our model, 25% of the nodes in the layer are randomly ignores, allowing the network to learn different features. This prevents overfitting.\n\n```relu``` is the rectifier, and it is used to find nonlinearity in the data. It works by returning the input value if the input value >= 0. If the input is negative, it returns 0.\n\nFlatten converts the tensors into a 1D vector.\n\nThe Dense layers are an artificial neural network (ANN). The last layer returns the probability that an image is in each class (one for each digit).\n\nAs this model aims to categorize the images, we will use a ```categorical_crossentropy``` loss function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape),\n    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    tf.keras.layers.MaxPool2D(strides=(2,2)),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])\n","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Fit the Training Data\n\nThe next step is to fit our training data. If we achieve a certain level of accuracy, it may not be necessary to continue training the model, especially if time and resources are limited.\n\nThe following cell defines a CallBack so that if 99.5% accuracy is achieved, the model stops training. The model is not likely to stop prematurely if only 5 epochs are specified. Try it out with more epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('acc')>0.995):\n      print(\"\\nReached 99.5% accuracy so cancelling training!\")\n      self.model.stop_training = True\n\ncallbacks = myCallback()","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing the model on a validation dataset prevents overfitting of the data. We specified a 10% validation and 90% training split."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_split=0.1,\n                    callbacks=[callbacks])","execution_count":12,"outputs":[{"output_type":"stream","text":"Train on 54000 samples, validate on 6000 samples\nEpoch 1/5\n54000/54000 [==============================] - 10s 186us/sample - loss: 0.2104 - acc: 0.9352 - val_loss: 0.0364 - val_acc: 0.9893\nEpoch 2/5\n54000/54000 [==============================] - 6s 111us/sample - loss: 0.0742 - acc: 0.9792 - val_loss: 0.0298 - val_acc: 0.9903\nEpoch 3/5\n54000/54000 [==============================] - 6s 112us/sample - loss: 0.0590 - acc: 0.9834 - val_loss: 0.0306 - val_acc: 0.9905\nEpoch 4/5\n54000/54000 [==============================] - 6s 107us/sample - loss: 0.0526 - acc: 0.9851 - val_loss: 0.0299 - val_acc: 0.9917\nEpoch 5/5\n54000/54000 [==============================] - 6s 107us/sample - loss: 0.0476 - acc: 0.9868 - val_loss: 0.0297 - val_acc: 0.9927\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 4. Evaluate the Model"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Loss and Accuracy Curves\n\nRun the following cell to evaluate the loss and accuracy of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training Accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation Accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":13,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-13-8f902b248cb7>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-8f902b248cb7>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")?\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"metadata":{},"cell_type":"markdown","source":"The accuracy increases over time and the loss decreases over time. However, the accuracy of our validation set seems to slightly decrease towards the end even thought our training accuracy increased. Running the model for more epochs might cause our model to be susceptible to overfitting."},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Predict Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model runs pretty well, with an accuracy of 99.3% on our testing data."},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Confusion Matrix\n\nRun the following cell to compute our confusion matrix using TensorFlow."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the values from the testing dataset\nY_pred = model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert testing observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1)\n# compute the confusion matrix\nconfusion_mtx = tf.math.confusion_matrix(Y_true, Y_pred_classes) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the following cell to plot the confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx, annot=True, fmt='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be a slightly higher confusion between (0,6) and (4,9). This is reasonable as 0's and 6's look similar with their loops and 4's and 9's can be mistaken when the 4's are more rounded and 9's are more angular."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}