{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "fz53jeLvDJz8"
   },
   "source": [
    "Adapted from [GitHub ACGAN Keras Example](https://github.com/keras-team/keras/blob/master/examples/mnist_acgan.py) and [GitHub Training Override Example](https://github.com/keras-team/keras-io/blob/master/examples/generative/dcgan_overriding_train_step.py)\n",
    "\n",
    "TensorFlow 1.x --> Tensorflow 2.x\n",
    "\n",
    "Uses GPU Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "1PWKHCVKbLGl"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This tutorial will go over the steps to build an ACGAN model on the MNIST dataset. The MNIST dataset is a dataset of images of handwritten digits 0 through 9. The ACGAN model, once trained, will generate fake images that resemble the real images of each class.\n",
    "\n",
    "Run the following cells to import the necessary packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0d-NrWjw7rtZ",
    "outputId": "b7e3788b-0f5a-4198-e52f-e7361bdb892f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from six.moves import range\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "_UsCLir5bvXP"
   },
   "source": [
    "For this tutorial, we will be focusing on the MNIST dataset. The MNIST dataset is a dataset of images of handwritten digits from 0 to 9. Since there are 10 digits, there are also 10 classes. The random seed is used so that the results are reproducible. We will load in our data after we define our methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YpB9PFMv9eO0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "num_classes = 10\n",
    "\n",
    "epochs = 30\n",
    "latent_dim = 128\n",
    "\n",
    "adam_lr = 0.0002\n",
    "adam_beta_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "IMKUPbtzb4wN"
   },
   "source": [
    "## 2. Building the generator\n",
    "\n",
    "The first step of building the GAN is to build the generator model. As the name implies, the generator model will be the part of our model that generates the images. The function that builds our generator is defined in the following cell using the TensorFlow Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FU0x1nxICLto"
   },
   "outputs": [],
   "source": [
    "def build_generator(latent_size):\n",
    "    cnn = tf.keras.Sequential()\n",
    "\n",
    "    cnn.add(layers.Dense(7 * 7 * 128, input_dim=latent_size))\n",
    "    cnn.add(layers.LeakyReLU(alpha=0.2))\n",
    "    cnn.add(layers.Reshape((7, 7, 128)))\n",
    "\n",
    "    cnn.add(layers.Conv2DTranspose(128, 4, strides=2, padding='same',\n",
    "                          kernel_initializer='glorot_normal'))\n",
    "    cnn.add(layers.LeakyReLU(alpha=0.2))\n",
    "    cnn.add(layers.BatchNormalization())\n",
    "\n",
    "    cnn.add(layers.Conv2DTranspose(128, 4, strides=2, padding='same',\n",
    "                          kernel_initializer='glorot_normal'))\n",
    "    cnn.add(layers.LeakyReLU(alpha=0.2))\n",
    "    cnn.add(layers.BatchNormalization())\n",
    "\n",
    "    cnn.add(layers.Conv2D(1, 7, padding='same',\n",
    "                          activation='tanh',\n",
    "                          kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "ByCBBzW8eEbN"
   },
   "source": [
    "## 3. Build the discriminator\n",
    "\n",
    "The second part of our generative model is our discriminator. The discriminator will be trained separately. It determines if the inputted image is a real or a fake image. The discriminator is used to train the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VfUA9kCiPlZk"
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    cnn = tf.keras.Sequential()\n",
    "\n",
    "    cnn.add(layers.Conv2D(32, 3, padding='same', strides=2,\n",
    "                          input_shape=(28, 28, 1)))\n",
    "    cnn.add(layers.LeakyReLU(0.2))\n",
    "    cnn.add(layers.Dropout(0.3))\n",
    "\n",
    "    cnn.add(layers.Conv2D(64, 3, padding='same', strides=1))\n",
    "    cnn.add(layers.LeakyReLU(0.2))\n",
    "    cnn.add(layers.Dropout(0.3))\n",
    "\n",
    "    cnn.add(layers.Conv2D(128, 3, padding='same', strides=2))\n",
    "    cnn.add(layers.LeakyReLU(0.2))\n",
    "    cnn.add(layers.Dropout(0.3))\n",
    "\n",
    "    cnn.add(layers.Conv2D(256, 3, padding='same', strides=1))\n",
    "    cnn.add(layers.LeakyReLU(0.2))\n",
    "    cnn.add(layers.Dropout(0.3))\n",
    "    \n",
    "    cnn.add(layers.GlobalMaxPooling2D()),\n",
    "    cnn.add(layers.Dense(1))\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Le9NeU20e5bd"
   },
   "source": [
    "Run the following cells to train the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LwMgNxICEQif",
    "outputId": "44921081-1472-4384-8624-fed349b47393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator model:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 388,097\n",
      "Trainable params: 388,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Discriminator model:')\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "uAavseKThfqM"
   },
   "source": [
    "## 3. Build the combined model\n",
    "\n",
    "Although the discriminator was trained separately, it's necessary to train the generator by using the discriminator. The generator will output an image and the discriminator will determine if the generated fake image is real or fake. The output of the discriminator will help train the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SpjkrRA1Fful",
    "outputId": "9b27bba1-85c9-4961-9be9-1beab662aa39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator model:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              809088    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 1,340,929\n",
      "Trainable params: 1,340,417\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Generator model:')\n",
    "generator = build_generator(latent_dim)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We are going to create a new class called GAN and it will be a subclass of the TensorFlow Keras Model class. By creating a new subclass, we can rewrite the ```train_step``` function, which will allow us to call ```model.fit()```. This reduces the code that we have to write to train this generative model and it allows for ease of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We will also create a subclass of the TensorFlow Keras Callback class called GANMonitor. Calling an instance of this subclass allos us to save a generated image at the end of each epoch to see how the model is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Let's now build our combined model. The generator is the model will be trained with the output of the discriminator. Additionally, since we have two ouputs, we want to measure both the binary crosentropy and the sparse cateogircal crossentropy as our losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BCbsiJzSH2Jw",
    "outputId": "aa4ebb38-67b2-4e0b-d9f2-094fbd609be5"
   },
   "outputs": [],
   "source": [
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "64BYVC8mi-w1"
   },
   "source": [
    "## 4. Load the data\n",
    "\n",
    "Now that the models have been defined and built, we have to load the data to train the model on. This tutorial will focus on the MNIST dataset. Luckily for us, the MNIST dataset can be easily accessed from the TensorFlow API.\n",
    "\n",
    "We want our data to be normalized to [0, 1]. As the data initially is between [0, 255], we will have to do some basic preprocessing and reshaping. Additionally, don't need a training and testing dataset because the images in both the the training and testing dataset are real images. Therefore, we can combine the two into a singular dataset to be used in our training.\n",
    "\n",
    "Run the next cell to load and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7Ha6-nk7KY9X",
    "outputId": "157ace8a-2449-483f-c45b-0e27e35689dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "all_digits = np.concatenate([x_train, x_test])\n",
    "all_digits = all_digits.astype(\"float32\") / 255\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "CBI80SHgj7g9"
   },
   "source": [
    "## 5. Train the model\n",
    "\n",
    "We previously specified the model to train for 30 epochs. Try training the model on more or less epochs to see the differences in loss and accuracy. The loss for our generative model and for our discriminator can be seen at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qhnXGvqmLBY5",
    "outputId": "6c66d3e4-0511-4681-9ff6-c5d74a026e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1094/1094 [==============================] - 23s 21ms/step - d_loss: 0.3825 - g_loss: 1.5732\n",
      "Epoch 2/30\n",
      "1094/1094 [==============================] - 23s 21ms/step - d_loss: 0.2990 - g_loss: 2.1589\n",
      "Epoch 3/30\n",
      "1094/1094 [==============================] - 23s 21ms/step - d_loss: 0.2685 - g_loss: 2.5588\n",
      "Epoch 4/30\n",
      "1094/1094 [==============================] - 22s 20ms/step - d_loss: 0.2399 - g_loss: 2.8778\n",
      "Epoch 5/30\n",
      "1094/1094 [==============================] - 23s 21ms/step - d_loss: 0.2336 - g_loss: 3.1397\n",
      "Epoch 6/30\n",
      "1094/1094 [==============================] - 22s 20ms/step - d_loss: 0.2105 - g_loss: 3.4694\n",
      "Epoch 7/30\n",
      "1094/1094 [==============================] - 22s 20ms/step - d_loss: 0.2034 - g_loss: 3.6134\n",
      "Epoch 8/30\n",
      "1094/1094 [==============================] - 22s 20ms/step - d_loss: 0.1828 - g_loss: 4.0456\n",
      "Epoch 9/30\n",
      "1094/1094 [==============================] - 22s 20ms/step - d_loss: 0.1770 - g_loss: 4.3203\n",
      "Epoch 10/30\n",
      "1094/1094 [==============================] - 23s 21ms/step - d_loss: 0.1665 - g_loss: 4.5156\n",
      "Epoch 11/30\n",
      "1094/1094 [==============================] - 22s 20ms/step - d_loss: 0.1647 - g_loss: 4.7074\n",
      "Epoch 12/30\n",
      "1094/1094 [==============================] - 22s 20ms/step - d_loss: 0.1474 - g_loss: 5.1701\n",
      "Epoch 13/30\n",
      "1094/1094 [==============================] - 23s 21ms/step - d_loss: 0.1901 - g_loss: 4.7204\n",
      "Epoch 14/30\n",
      " 471/1094 [===========>..................] - ETA: 12s - d_loss: 0.1730 - g_loss: 4.7050"
     ]
    }
   ],
   "source": [
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "bueGGvxSkjef"
   },
   "source": [
    "## 6. Visualize the generated images\n",
    "\n",
    "Run the following cell to visualize some of the saved generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb\tgenerated_img_1_0.png\tgenerated_img_2_1.png\r\n",
      "generated_img_0_0.png\tgenerated_img_1_1.png\tgenerated_img_2_10.png\r\n",
      "generated_img_0_1.png\tgenerated_img_1_10.png\tgenerated_img_2_11.png\r\n",
      "generated_img_0_10.png\tgenerated_img_1_11.png\tgenerated_img_2_12.png\r\n",
      "generated_img_0_11.png\tgenerated_img_1_12.png\tgenerated_img_2_13.png\r\n",
      "generated_img_0_12.png\tgenerated_img_1_13.png\tgenerated_img_2_14.png\r\n",
      "generated_img_0_13.png\tgenerated_img_1_14.png\tgenerated_img_2_15.png\r\n",
      "generated_img_0_14.png\tgenerated_img_1_15.png\tgenerated_img_2_16.png\r\n",
      "generated_img_0_15.png\tgenerated_img_1_16.png\tgenerated_img_2_17.png\r\n",
      "generated_img_0_16.png\tgenerated_img_1_17.png\tgenerated_img_2_18.png\r\n",
      "generated_img_0_17.png\tgenerated_img_1_18.png\tgenerated_img_2_19.png\r\n",
      "generated_img_0_18.png\tgenerated_img_1_19.png\tgenerated_img_2_2.png\r\n",
      "generated_img_0_19.png\tgenerated_img_1_2.png\tgenerated_img_2_20.png\r\n",
      "generated_img_0_2.png\tgenerated_img_1_20.png\tgenerated_img_2_21.png\r\n",
      "generated_img_0_20.png\tgenerated_img_1_21.png\tgenerated_img_2_22.png\r\n",
      "generated_img_0_21.png\tgenerated_img_1_22.png\tgenerated_img_2_23.png\r\n",
      "generated_img_0_22.png\tgenerated_img_1_23.png\tgenerated_img_2_24.png\r\n",
      "generated_img_0_23.png\tgenerated_img_1_24.png\tgenerated_img_2_25.png\r\n",
      "generated_img_0_24.png\tgenerated_img_1_25.png\tgenerated_img_2_26.png\r\n",
      "generated_img_0_25.png\tgenerated_img_1_26.png\tgenerated_img_2_27.png\r\n",
      "generated_img_0_26.png\tgenerated_img_1_27.png\tgenerated_img_2_28.png\r\n",
      "generated_img_0_27.png\tgenerated_img_1_28.png\tgenerated_img_2_29.png\r\n",
      "generated_img_0_28.png\tgenerated_img_1_29.png\tgenerated_img_2_3.png\r\n",
      "generated_img_0_29.png\tgenerated_img_1_3.png\tgenerated_img_2_4.png\r\n",
      "generated_img_0_3.png\tgenerated_img_1_4.png\tgenerated_img_2_5.png\r\n",
      "generated_img_0_4.png\tgenerated_img_1_5.png\tgenerated_img_2_6.png\r\n",
      "generated_img_0_5.png\tgenerated_img_1_6.png\tgenerated_img_2_7.png\r\n",
      "generated_img_0_6.png\tgenerated_img_1_7.png\tgenerated_img_2_8.png\r\n",
      "generated_img_0_7.png\tgenerated_img_1_8.png\tgenerated_img_2_9.png\r\n",
      "generated_img_0_8.png\tgenerated_img_1_9.png\r\n",
      "generated_img_0_9.png\tgenerated_img_2_0.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zc57XhtmaJR4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB30lEQVR4nHWSzWsUQRDFX1XP17qCIVEI4kWCi2iCIipsPHiSiAbBDxACIl7EP0DwmlsIiIp6CGwOegqCIAoqXhTRg4niQdQgfpyElSiiZN3MONP9PGRmdwykDt0Nr3/V9apatmD10NJZVhUFoIhASndKpAiB/1TtgCIEBUC+AvDy3R44uStqp9WsOX9xgSvIvpGtQfo9NqZv5IIUpORWZOOGF9Hg7uN+qzXzUFekxdcfgdd8Ut80e/ejAYUlUSg7T7Ti7UPf3r1cbx1YSitAdWYgNUaR1NMU6tgtiAzGa17kKxC+njuydmnZa04ymguxNPUgOTvaA9pPx+DAgpRDvrtfu9b8daV3YPS3Vzvt2CXd9b3T02raYRrFoX1VjcfmXf6mcB0ak4I0sh6ZmVuMzkNykfjzqNErVjPfZkHGZ4SyWy0WnDq1oFh45pTicWFF4OpHQxrngRom/rZ9SG+XfB7cUcky45RIqkNToTu3CAAqAKGf2+o8Eh6D/Td6eOe5AQElBGI/VIYXEQdJZfDe1QBvxgUdn8LNNznbwJk9/Ubh3o5lXVFInTgcUAAkzctP/5LlqUDS4Yl++/P9pS9OUPySTuOhadAK/OUpF6qXj5rOcA0IJx2u0z4C4vJmdeMf09nH/YmLl6wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7FBCA00AC8D0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(\"generated_img_2_20.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
