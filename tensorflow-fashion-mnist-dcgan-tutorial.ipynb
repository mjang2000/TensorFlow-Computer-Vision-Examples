{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "jOl58C4hyXhj"
   },
   "source": [
    "Adapted from [Machine Learning Mastery Article](https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/) and [Keras Training Override Example](https://github.com/keras-team/keras-io/blob/master/examples/generative/dcgan_overriding_train_step.py)\n",
    "\n",
    "\n",
    "TensorFlow 1.x --> TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "TFApRmAWSs9u"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial will go over how to train an ACGAN (auxiliary classifier generative adversarial network) using the Fashion MNIST dataset. An ACGAN model will generate images for a given condition, with the condition being a class.\n",
    "\n",
    "The Fashion MNIST dataset is a datset consistng of 60000 training examples. Each example is a 28x28 image of an article of clothing, with each example falling under one of 10 classes.\n",
    "\n",
    "The classes are as follows:\n",
    "0. T-shirt\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n",
    "\n",
    "Run the following cell to download the necessary packages. If running on a TPU, change the ```TPU_used``` variable to true. Else, change the accelerator on the right to GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0lfnc40pyC3D",
    "outputId": "db71c2f9-204b-4851-9f53-fadd91d3ba48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "TPU_used = False\n",
    "\n",
    "if TPU_used:\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "    except ValueError:\n",
    "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "The random seed allows for the notebook to be reproducible. As there are the 10 classes in the Fashion MNIST dataset, we set the number of classes to to 10. The number of epochs is initially set to 30 but feel free to change the number of epochs. The learning rate and beta value will be used later when we compile our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "num_classes = 10\n",
    "\n",
    "epochs = 30\n",
    "latent_dim = 128\n",
    "\n",
    "adam_lr = 0.0002\n",
    "adam_beta_1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "inBfmYCIYOz2"
   },
   "source": [
    "## 1. Importing data\n",
    "\n",
    "The following cell defines a function to import the Fashion MNIST dataset. As we are building a generative model, we don't need a separate testing set and all the images in the datset will be used as real images for our model.\n",
    "\n",
    "The data is scaled so that the data is normalized to [0, 1] rather than [0, 255]. Normalizing the data is an important part of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 3us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "(x_train, _), (x_test, _) = load_data()\n",
    "all_images = np.concatenate([x_train, x_test])\n",
    "all_images = all_images.astype(\"float32\") / 255\n",
    "all_images = np.reshape(all_images, (-1, 28, 28, 1))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(all_images)\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "QnKzpha4ZfzN"
   },
   "source": [
    "## 2. Building the discriminator\n",
    "\n",
    "To build a GAN, a discriminator model must first be built. The discriminator takes an image in as its input and returns the probability that the image is real.\n",
    "\n",
    "We define the method to build our discriminator in the following cell using the TensorFlow Keras API.\n",
    "\n",
    "Conv2D is a convolution layer that applies a filter of a specified size (in this example, a 3x3 kernel, on each pixel). This allows for certain features to stand out.\n",
    "\n",
    "LeakyRelu is similar to a rectifier, but instead of having all negative values become 0, there is a small negative slope. This layer allows the model to find nonlinearities.\n",
    "\n",
    "Dropout randomly ignores 0.5 of the input nodes. This prevents overfitting  by forcing to model to learn new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32, 3, strides=2, padding='same',\n",
    "                          input_shape=(28, 28, 1)),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Conv2D(64, 3, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Conv2D(128, 3, strides=2, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Conv2D(256, 3, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.GlobalMaxPooling2D(),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VkvL6Vze1h0c",
    "outputId": "46a3f905-bc21-4099-d991-99e79353ec49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 389,889\n",
      "Trainable params: 388,993\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if TPU_used:\n",
    "    with tpu_strategy.scope():\n",
    "        discriminator = define_discriminator()\n",
    "else:\n",
    "    discriminator = define_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "6B0S_etXb7lu"
   },
   "source": [
    "## 4. Building the generator\n",
    "\n",
    "Like the discriminator, the generator must also be built before we can move on to our GAN model.\n",
    "\n",
    "The generator take in a random point from the latent space and a class label, and it returns a generated image that falls under the specified class label. A latent space is a way to represent condensed information in a way that similar datapoints have smaller distances between them.\n",
    "\n",
    "A point in the latent space can by used to create multiple 7x7 feature maps, and these maps, along with the feature map created by the class label, can be upcaled to a 14x14 and then a 28x28 image.\n",
    "\n",
    "Because the generator is trained with the discriminator, it should not be compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_size):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(7 * 7 * 128, input_dim=latent_size),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Reshape((7, 7, 128)),\n",
    "            \n",
    "            layers.Conv2DTranspose(128, 4, strides=2, padding='same',\n",
    "                                   kernel_initializer='glorot_normal'),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(),\n",
    "            \n",
    "            layers.Conv2DTranspose(128, 4, strides=2, padding='same',\n",
    "                                   kernel_initializer='glorot_normal'),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(),\n",
    "            \n",
    "            layers.Conv2D(1, 7, padding='same',\n",
    "                          activation='tanh',\n",
    "                          kernel_initializer='glorot_normal')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FBP3t2as9uEy",
    "outputId": "11a265c3-a735-480f-e143-fde07693bdc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              809088    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 1,340,929\n",
      "Trainable params: 1,340,417\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if TPU_used:\n",
    "    with tpu_strategy.scope():\n",
    "        generator = define_generator(latent_dim)\n",
    "else:\n",
    "    generator = define_generator(latent_dim)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "WMExu--WgzDb"
   },
   "source": [
    "## 5. Buiding the composite model.\n",
    "\n",
    "As mentioned previously, the generator is trained using the discriminator model. The composite model will take in the same input as the generator, feed it into the generator, and the generated image will be fed into the discriminator. During training, we do not want to update the weights in the discriminator. The discriminator will be trained separately from within the composite model.\n",
    "\n",
    "We are going to create a new class called GAN and it will be a subclass of the TensorFlow Keras Model class. By creating a new subclass, we can rewrite the train_step function, which will allow us to call ```model.fit()```. This reduces the code that we have to write to train this generative model and it allows for ease of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FVO8ZjrN9zPg"
   },
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We will also create a subclass of the TensorFlow Keras Callback class called GANMonitor. Calling an instance of this subclass allows us to save a generated image at the end of each epoch to see how the model is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Let's now build our combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TPU_used:\n",
    "    with tpu_strategy.scope():\n",
    "        gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "        gan.compile(\n",
    "            d_optimizer=tf.keras.optimizers.Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n",
    "            g_optimizer=tf.keras.optimizers.Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n",
    "            loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                                       reduction=tf.keras.losses.Reduction.NONE),\n",
    "        )\n",
    "else:\n",
    "    gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "    gan.compile(\n",
    "        d_optimizer=tf.keras.optimizers.Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n",
    "        g_optimizer=tf.keras.optimizers.Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n",
    "        loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "E8QIrmkoCVq6"
   },
   "source": [
    "## 6. Train the model\n",
    "\n",
    "The following function defines how the model is going to be trained. It is trained with a batch, half of which is real images and the other half is fake images created by the generator.\n",
    "\n",
    "Run the following cell to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CF6W33RqB1VD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.5916 - g_loss: 1.1209\n",
      "Epoch 2/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.5379 - g_loss: 1.2438\n",
      "Epoch 3/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.5355 - g_loss: 1.2579\n",
      "Epoch 4/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.5218 - g_loss: 1.2770\n",
      "Epoch 5/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.5309 - g_loss: 1.2713\n",
      "Epoch 6/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.4957 - g_loss: 1.3081\n",
      "Epoch 7/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.4907 - g_loss: 1.3133\n",
      "Epoch 8/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.4907 - g_loss: 1.3133\n",
      "Epoch 9/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.4907 - g_loss: 1.3133\n",
      "Epoch 10/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.4907 - g_loss: 1.3133\n",
      "Epoch 11/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.4907 - g_loss: 1.3133\n",
      "Epoch 12/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.4907 - g_loss: 1.3133\n",
      "Epoch 13/30\n",
      "1094/1094 [==============================] - 24s 22ms/step - d_loss: 0.4907 - g_loss: 1.3133\n",
      "Epoch 14/30\n",
      " 306/1094 [=======>......................] - ETA: 17s - d_loss: 0.4908 - g_loss: 1.3133"
     ]
    }
   ],
   "source": [
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "hO-gZX0NCbbg"
   },
   "source": [
    "## 7. Visualize images using created by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qHBQ4WWkp3-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb\tgenerated_img_1_0.png\tgenerated_img_2_1.png\r\n",
      "generated_img_0_0.png\tgenerated_img_1_1.png\tgenerated_img_2_10.png\r\n",
      "generated_img_0_1.png\tgenerated_img_1_10.png\tgenerated_img_2_11.png\r\n",
      "generated_img_0_10.png\tgenerated_img_1_11.png\tgenerated_img_2_12.png\r\n",
      "generated_img_0_11.png\tgenerated_img_1_12.png\tgenerated_img_2_13.png\r\n",
      "generated_img_0_12.png\tgenerated_img_1_13.png\tgenerated_img_2_14.png\r\n",
      "generated_img_0_13.png\tgenerated_img_1_14.png\tgenerated_img_2_15.png\r\n",
      "generated_img_0_14.png\tgenerated_img_1_15.png\tgenerated_img_2_16.png\r\n",
      "generated_img_0_15.png\tgenerated_img_1_16.png\tgenerated_img_2_17.png\r\n",
      "generated_img_0_16.png\tgenerated_img_1_17.png\tgenerated_img_2_18.png\r\n",
      "generated_img_0_17.png\tgenerated_img_1_18.png\tgenerated_img_2_19.png\r\n",
      "generated_img_0_18.png\tgenerated_img_1_19.png\tgenerated_img_2_2.png\r\n",
      "generated_img_0_19.png\tgenerated_img_1_2.png\tgenerated_img_2_20.png\r\n",
      "generated_img_0_2.png\tgenerated_img_1_20.png\tgenerated_img_2_21.png\r\n",
      "generated_img_0_20.png\tgenerated_img_1_21.png\tgenerated_img_2_22.png\r\n",
      "generated_img_0_21.png\tgenerated_img_1_22.png\tgenerated_img_2_23.png\r\n",
      "generated_img_0_22.png\tgenerated_img_1_23.png\tgenerated_img_2_24.png\r\n",
      "generated_img_0_23.png\tgenerated_img_1_24.png\tgenerated_img_2_25.png\r\n",
      "generated_img_0_24.png\tgenerated_img_1_25.png\tgenerated_img_2_26.png\r\n",
      "generated_img_0_25.png\tgenerated_img_1_26.png\tgenerated_img_2_27.png\r\n",
      "generated_img_0_26.png\tgenerated_img_1_27.png\tgenerated_img_2_28.png\r\n",
      "generated_img_0_27.png\tgenerated_img_1_28.png\tgenerated_img_2_29.png\r\n",
      "generated_img_0_28.png\tgenerated_img_1_29.png\tgenerated_img_2_3.png\r\n",
      "generated_img_0_29.png\tgenerated_img_1_3.png\tgenerated_img_2_4.png\r\n",
      "generated_img_0_3.png\tgenerated_img_1_4.png\tgenerated_img_2_5.png\r\n",
      "generated_img_0_4.png\tgenerated_img_1_5.png\tgenerated_img_2_6.png\r\n",
      "generated_img_0_5.png\tgenerated_img_1_6.png\tgenerated_img_2_7.png\r\n",
      "generated_img_0_6.png\tgenerated_img_1_7.png\tgenerated_img_2_8.png\r\n",
      "generated_img_0_7.png\tgenerated_img_1_8.png\tgenerated_img_2_9.png\r\n",
      "generated_img_0_8.png\tgenerated_img_1_9.png\r\n",
      "generated_img_0_9.png\tgenerated_img_2_0.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sPc7i6j0N9J3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACcUlEQVR4nEWQW08TURSF1z5zpqczbSl0BrW0UhpRUzHiNcagv8BH/5J/xidffTbRaIyN0US5KdAqLQg17TCdy5k52wdAn1e+vda36TkDJuJUcqAWd6pzQ9fMFQ+kAAgSYGOkOvKCsHXp5Eo52606LTEkAJDMxpiK30oPFQm9Moj0jOsnQwaIBYCssujZYbHq9YOv2R/pTt7NyBwAJADVKB37etN5bYXdRrI4uj02qaMJEAAXZ9XHDzllo+Kgq5tHg2SBHWEDEAAmteUlP8/cO8pq1h2nXqjtCe0AEARR04X7E5UGvcMoiL6E5eNxaiv79KzRW/ruU/Ow0IlvTNcvr7YbviFJp4Mo/j687+Oot33h89ZCfPnLrUjGnDJBMMAmeZfcejB/rcLzjYMT3bHnJ2l82klUNCcv92aXfqBvXZSyaKuyyAkMyQDBFnF50lno3Znd8AvRXt0SAiAWBIBIyKAweF0cDBJW7k4SEnDmCQOYatxpqLq+kW1V0+Pc8HnIxDBs/wmz6dG3fbuWZPqUlACIwU45Tdq7j7qq49Wceg46C4kJZOnC+E1je98fhaYtwv8kmLLl0sgva9Vs6+hk6dc0t+g8JLBT2Q2j8brsjdaUseJCDjpby4avcrDRUTNSzTaaTS+2AP5HKrfU/36YdXXpoBLfc6Y492RwXgnC/YuPtbfSqv+izbSQEgAIBkDKZefzxqefu8P6TvWwNNNhBgBJTIw0U8UH1vXfa1vduw5Zc+33ggEIEAAaL5Q6ZvlZOV+7+aTmei1p/nuKqN+ac3tXvP3VaanuRTELPvsQCPSq+vb4xUo32ObVfq+8CiaA/gJMvyWT2qpyAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7FA3A0119E10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(\"generated_img_2_20.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
